= HDFS Connector
:keywords: anypoint studio, esb, connectors, hdfs
:imagesdir: ./_images
:toc: macro
:toclevels: 2

toc::[]

== Introduction

The Anypoint Connector for the Hadoop Distributed File System (HDFS) is used as a bi-directional gateway between Mule applications and HDFS.

Read through this user guide to understand how to set up and configure a basic flow using the connector. Track feature additions, compatibility, limitations and API version updates with each release of the connector using the link:/release-notes/hdfs-connector-release-notes[Connector Release Notes]. Review the connector operations and functionality using the link:/mulesoft.github.io/hdfs-connector[Technical Reference] alongside the link:https://www.mulesoft.com/exchange#!/?filters=HDFS&sortBy=rank[demo applications].

== Prerequisites

This document assumes that you are familiar with Mule.

To use the HDFS connector, you need:

* *Anypoint Studio* version 7.0 or above - An instance of link:https://www.mulesoft.com/lp/dl/mule-esb-enterprise[Anypoint Studio]. If you do not use Anypoint Studio for development, follow the instructions in <<Configuring Maven Dependencies,Configuring Maven Dependencies>> for your project.
* An instance of Hadoop Distributed File System  up and running. It can be downloaded from link:http://hadoop.apache.org/releases.html[here].

== Compatibility

HDFS Hadoop connector is compatible with the following:

[%header,width="100%",cols="50%,50%"]
|===
|Application/Service|Version
|Mule Runtime |4.0 or newer
|Apache Hadoop |2.8.1 or newer
|===

== What's New in this Connector

First release on Mule 4.


== To Connect in Design Center

. In Design Center, click Set Up > Upload, browse for and select the driver for this connector on your file system, and upload it. Alternatively, search for and select a driver that is already uploaded.
. Click a trigger. You can create a global element by selecting this connector as they trigger.
If a global element is not needed, you can use an HTTP Listener or Scheduler trigger.

image::trigger.png[]

To create an HTTP global element for the connector, set these fields:

. Select the plus sign to add a component.
. Select the Http connector as a component.
. Configure these fields:


image::http-listener.png[]

Protocol: Protocol selected for the http endpoint, it can be http or https (secure).

Host: IP address where our Mule Application will listen requests.

Port: Port address where our Mule Application will listen requests.

Base Path: path where our Mule Application will listen requests.

Select the plus sign to add a component.

image::plus.png[]

Select the connector as a component.

Select the desired operation.

image::operations.png[]

Configure the Global element for the connector.

Select a connection type.

image::connection-type.png[]

For Simple Connection:

image::simple-connection.png[]

Username: User identity that Hadoop uses for permissions in HDFS.


NameNod URI: The URI of the file system to connect to.

For Kerberos Configuratin:

image::kerberos-connection.png[]

== Configure the Connector Global Element

To use the HDFS connector in your Mule application, you must configure a global HDFS element that can be used by the connector (read more about  link:/mule-fundamentals/v/4.0/global-elements[Global Elements]). The HDFS connector offers the following global configuration options, requiring the following credentials:

=== Simple Authentication Configuration

[%header,width="100a",cols="50a,50a"]
|===
|Field |Description
|*NameNode URI* |The URI of the file system to connect to.
[NOTE]
This is passed to the HDFS client as the *FileSystem#FS_DEFAULT_NAME_KEY* configuration entry. It can be overridden by values in configurationResources and configurationEntries.
|*Username* | User identity that Hadoop uses for permissions in HDFS.
[NOTE]
When Simple Authentication is used, Hadoop requires the user to be set as a System Property called HADOOP_USER_NAME. If you fill this field then the connector will set it for you, however you can set it by yourself. If the variable is not set, Hadoop will use the current logged in OS user.
|*Configuration Resources* |A list of configuration resource files to be loaded by the HDFS client. Here you can provide additional configuration files. (e.g core-site.xml)
|*Configuration Entries* |A map of configuration entries to be used by the HDFS client. Here you can provide additional configuration entries as key/value pairs.
|===

image:hdfs-config.png[hdfs-config]

NameNode URI: The URI of the file system to connect to.

Username: Kerberos principal.

KeytabPath: Path to the keytab file associated with username.

=== Kerberos Authentication Configuration

[%header,width="100a",cols="50a,50a"]
|===
|Field |Description
|*NameNode URI* |The URI of the file system to connect to.
[NOTE]
This is passed to HDFS client as the *FileSystem#FS_DEFAULT_NAME_KEY* configuration entry. It can be overridden by values in configurationResources and configurationEntries.
|*Username* | Kerberos principal.
[NOTE]
This is passed to HDFS client as the "hadoop.job.ugi" configuration entry. It can be overridden by values in configurationResources and configurationEntries. If not provided it will use the currently logged in user.
|*KeytabPath* |Path to the link:https://web.mit.edu/kerberos/krb5-1.12/doc/basic/keytab_def.html[keytab file] associated with username.
[NOTE]
KeytabPath is used in order to obtain TGT from "Authorization server".  If not provided it will look for a TGT associated to username within your local kerberos cache.
|*Configuration Resources* |A list of configuration resource files to be loaded by the HDFS client. Here you can provide additional configuration files. (e.g core-site.xml)
|*Configuration Entries* |A map of configuration entries to be used by the HDFS client. Here you can provide additional configuration entries as key/value pairs.
|===

image:hdfs-config-with-kerberos.png[hdfs-config-with-kerberos]

== Using the Connector

You can use this connector as an inbound endpoint for polling content of a file at a configurable rate (interval) or as an outbound connector for manipulating data into the HDFS server.

[NOTE]
See a full list of operations for any version of the connector link:/mulesoft.github.io/hdfs-connector[here].

=== Connector XML

When designing your application in Studio, the act of dragging the connector from the palette onto the Anypoint Studio canvas should automatically populate the XML code with the connector *namespace* and *schema location*.

*Namespace:* `http://www.mulesoft.org/schema/mule/hdfs` +
*Schema Location:* `http://www.mulesoft.org/schema/mule/connector/current/mule-hdfs.xsd`

[TIP]
If you are manually coding the Mule application in Studio's XML editor or other text editor, define the namespace and schema location in the header of your *Configuration XML*, inside the `<mule>` tag.

[source, xml,linenums]
----
 <mule xmlns:http="xmlns:hdfs="http://www.mulesoft.org/schema/mule/hdfs"
      	xmlns:ee="http://www.mulesoft.org/schema/mule/ee/core"
      	xmlns="http://www.mulesoft.org/schema/mule/core" xmlns:doc="http://www.mulesoft.org/schema/mule/documentation"
      	xmlns:spring="http://www.springframework.org/schema/beans"
      	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      	xsi:schemaLocation="

      http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
      http://www.mulesoft.org/schema/mule/ee/core http://www.mulesoft.org/schema/mule/ee/core/current/mule-ee.xsd
      http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
      http://www.mulesoft.org/schema/mule/hdfs http://www.mulesoft.org/schema/mule/hdfs/current/mule-hdfs.xsd">

    <hdfs:hdfs-config name="simple-config">
        <hdfs:simple-connection nameNodeUri="${hdfs.nameNodeUri}" username="${hdfs.username}"/>
    </hdfs:hdfs-config>

     <flow name="makeDirFlow">
        <hdfs:make-directories config-ref="simple-config" path="#[vars.path]" permission="#[vars.permission]">
        </hdfs:make-directories>
      </flow>
</mule>
----
=== Example Use Case

The following example shows how to create a text file into HDFS using the connector:

. In Anypoint Studio, click *File* > *New* > *Mule Project*, name the project, and click *OK*.
. In the search field, type "http" and drag the *HTTP* connector to the canvas, click the green plus sign to the right of *Connector Configuration*, and in the next screen, click *OK* to accept the default settings. Name the endpoint */createFile*.
. In the Search bar type "HDFS" and drag the *HDFS* connector onto the canvas. Configure as explained <<Configure the Connector Global Element>>
. Choose *Write to path* as an operation. Set *Path* to `/test.txt` (this is the path of the file that is going to be created into HDFS) and leave other options with default values.
. The flow should look like this:

image:create-file-flow.png[Create file flow]

. Run the application. From your favorite HTTP client make a POST request with "Content-type:plain/text" to `locahost:8081/createFile` with content that you want to write as payload. (e.g. `curl -X POST -H "Content-Type:plain/text" -d "payload to write to file" localhost:8090/createFile`)
. Check that */test.txt* has been created and has your content by using Hadoop explorer.
* A demo with the above mentioned use case was provided.

== Resources

* Access the link:/release-notes/hdfs-connector-release-notes[HDFS Connector Release Notes].
